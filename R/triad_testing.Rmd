---
title: "Triad testing for ChatGPT"
author: "Jasper Robbins"
date: "2023-08-21"
---

```{r setup, include=FALSE,echo=FALSE,warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(httr)
library(jsonlite)
library(dplyr)
library(stringr)
library(future)
library(future.apply)
source("./functions/processing_helpers.R")
source("./functions/gpt_helpers.R")
```


## Add your API Key here
```{r setup api key}
# Replace this API key with the one you received when registering.
api_key <- "sk-..."
```

## Read data from file
```{r import data}
triad_data <- process_triad_data(
  "your_file_path"
)
```


## Edit the prompt template
```{r prompt template}

# Format a triad string for each row of format: '{word_A,word_B,word_C};'
triad_data <- triad_data %>%
  rowwise %>%
  mutate(
    triad = paste0("{", paste(A, B, C, sep = ","), "};", sep = "")
  )

# Update your prompt to ChatGPT here.
prompt_start <- "We are going to present you with three words. Your task is to analyse the meaning of all possible word pairs in the triad, and subsequently report the probability that you would choose each pair in the triad as more similar than the others. Your choice is to be based solely in terms of semantic word meaning. This means that phonemic or visual similarity of the words should not be factored into your choice.   
Here are the steps to complete the analysis:
Step 1: Closely scrutinize every possible pair in the triad.
Step 2: After detailed consideration, report the pair you deem most semantically similar.
You will now be provided with two examples. Note that the specific probabilities of the provided examples are arbitrary for the sake of the example, and the probabilities in the example are not indicative of their true semantic similarity with no explanation.
Example 1:
       	Presented: {Angel; Halo; Plate}
       	Response: {Angel, Halo}
Example 2:
Presented: {Hair; Cage; Cheap}
        	Response: {Cage, Cheap}
Now that you have been provided with these examples, you are ready to begin the task. Please take as much time as possible to consider all possible word pairings before making a decision on the probabilities.
Here is the triad:
"
prompt_end <- ""

```

## NOTE - If you want to ask chatgpt to return the most similar pair, set similarity_scores to false. If you want to ask for similarity ratings for every word pair, set similarity_scores to true. This requires chatgpt to respond with {word_1 word_2 similarity_rating} for each triad.

## Adjust specs and generate prompts
```{R prompt building}

# Specify how many triads_per_prompt you want to ask per prompt
triads_per_prompt = 1

# Specify if using similarity scores for word-pairs
similarity_scores = FALSE

# Optionally filter triads, e.g.
triad_data <- triad_data %>% filter(dataset %in% c("abstract"))

# Generate a dataframe of prompts
prompts = generate_prompts(
  triad_data,
  triads_per_prompt, 
  prompt_start, 
  prompt_end
)

num_prompts = nrow(prompts)
cat("You have", num_prompts, "prompts.")
```


## Adjust gpt settings and check cost for full dataset
```{r gpt costing}
# Choose your gpt model
gpt_model <- "gpt-3.5-turbo" # or "gpt-4"

# I believe increases the variance between answers
gpt_temperature <- 0

# Check the cost
estimate_cost(
  gpt_model,
  nrow(prompts),
  paste(prompt_start, prompt_end),
  triads_per_prompt
)
```

## Query ChatGPT on a single prompt
```{r test query}
# Send one prompt request to ChatGPT
response <- ask_gpt(prompts[1, ], gpt_model, gpt_temperature, api_key)

# Process response and output a tibble
print(process_response(response, similarity_scores))

```



```{r}
# This allows multi-processing using the number of cores that your cpu has
plan(multisession)
num_cores = parallel::detectCores()

# OpenAI likely limits how many requests you can send at once, I've found 25
# at once works but feel free to push the limits (although diminishing returns)
rate_limit = 25

totals <- tibble()

for (i in seq(1, floor(100 / rate_limit))) {
  
  # Get the indexes for each batch (e.g., 1-20, 21-40...)
  idxs <- seq(1 + (i-1)*rate_limit, (i)*rate_limit)
  batch <- prompts[idxs, ]
  cat(idxs[1], "-", idxs[rate_limit], "... ")
  
  # Send requests to GPT
  responses <- value(
    future_lapply(
        batch, 
        function(prompt) ask_gpt( # ask_gpt is my function to send a request
          prompt = prompt,
          model = gpt_model,
          temperature = gpt_temperature,
          api_key = api_key
        )
      )
  )
  
  # Clean up responses (with or without similarity ratings)
  processed_messages <- lapply(
    responses, 
    function(item) process_response(
      item,
      similarity_scores
    )
  )
  
  # Process responses
  processed_answers <- sapply(processed_messages, function(gpt_message) {
    
    # If using similarity scores, the 'answer' is the highest similarity pair
    if (similarity_scores) {
      answers <- gpt_message %>% 
        group_by(group = (row_number() - 1) %/% 3) %>%
        filter(similarity == max(similarity)) %>%
        distinct(similarity, .keep_all = TRUE) %>%
        ungroup() %>%
        select(-group, -similarity)
    } else {
      answers <- gpt_message[[1]]
    }
    return(answers[[1]])
  }, simplify = "matrix") %>% 
    as_tibble()
  
  # Store answers
  totals <- bind_rows(totals, processed_answers)
}

# Cherrypick initial data
data <- triad_data %>%
  select(A, B, C, difficulty, human_coded_answer, dataset, category) %>%
  add_column(
    gpt_answer = NA,
    gpt_coded_answer = NA,
    correct = NA,
    gpt_message = NA,
    position = NA
  )

# Save results
data$gpt_answer <- totals[[1]]

# For each triad...
for (j in seq(1, nrow(data))) {

  # Convert the answer into code AB/AC/BC
  coded_answer <- str_c(
    ifelse(grepl(data$A[j], data$gpt_answer[j]), "A", ""),
    ifelse(grepl(data$B[j], data$gpt_answer[j]), "B", ""),
    ifelse(grepl(data$C[j], data$gpt_answer[j]), "C", ""),
    sep = ""
  )
  data$gpt_coded_answer[j] <- coded_answer

  # Check if answer is correct
  data$correct[j] <- grepl(coded_answer, data$human_coded_answer[j])
  
  # Store extra response data
  data$gpt_message[j] <- content(response)$choices[[1]]$message$content
  data$position[j] <- (j - 1) %% triads_per_prompt + 1
}

```

## Process and summarise results
```{r results}
datafalse <- data %>%
  filter(correct == "FALSE") %>%
  select(
    correct,
    dataset,
    category,
    A, B, C,
    human_coded_answer,
    gpt_coded_answer,
    gpt_answer,
  )

percentage_by_category <- data %>%
  group_by(category) %>%
  summarise(
    accuracy = round(sum(correct, na.rm = TRUE) / n(), 2),
    count = n(),
  ) %>%
  select(category, accuracy, count)

percentage_by_answer <- data %>%
  group_by(human_coded_answer) %>%
  summarise(
    accuracy = round(sum(correct, na.rm = TRUE) / n(), 2),
    count = n(),
  ) %>%
  select(human_coded_answer, accuracy, count)

summary_stats <- tibble(
  "------OVERALL------" = "----------------------------",
  score = paste(sum(data$correct, na.rm = TRUE), "/", nrow(triad_data)),
  percentage = paste(
    100 * sum(data$correct, na.rm = TRUE) / nrow(triad_data),
    "%"
  ),
  "-------OTHER-------" = "----------------------------",
  avg_correct_difficulty = round(
    mean(data$difficulty[data$correct], na.rm = TRUE), 
    2
  ),
  avg_incorrect_difficulty = round(
    mean(data$difficulty[!data$correct], na.rm = TRUE), 
    2
  ),
  "AB_%" = percentage_by_answer$accuracy[
    percentage_by_answer$human_coded_answer == "AB,"
  ],
  "AC_%" = percentage_by_answer$accuracy[
    percentage_by_answer$human_coded_answer == "AC,"
  ],
  "BC_%" = percentage_by_answer$accuracy[
    percentage_by_answer$human_coded_answer == "BC,"
  ],
) %>%
  mutate(across(everything(), toString)) %>%
  pivot_longer(
    everything(),
    names_to = "stat",
    values_to = "value"
  )

# % by correct by the category of the triad in prompt
print(percentage_by_category)

# % by correct by the code of the triad answer
print(percentage_by_answer)

# Incorrect response data
print(datafalse)

# Overall summary stats
print(summary_stats)
```